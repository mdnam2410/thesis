\section{Overview of Explainable Artificial Intelligence}
\label{sec:xaiOverview}
eXplainable Artificial Intelligence (XAI) is not a new concept that has emerged recently. The earliest research on XAI can be traced back to literature published four decades ago \cite{explanation, explainingPrograms}, when certain expert systems were designed to provide explanations for their outputs based on the rules they applied. Scientists have been engaged in continuous discussions about the significance of explanations in intelligent systems, specifically concerning decision-making, since the beginning of AI research.

In the context of modern deep learning, however, XAI has emerged as a novel research area in response to the increasing complexity and opacity of machine learning models. It aims to provide insights into AI systems' decision-making processes and enable users to understand, trust, and effectively interact with these systems. The advent of XAI can be traced back to the early 2000s when Lipton raised a renewed interest in the field, emphasizing the importance of explainability in machine learning algorithms by his influence paper in 2016 \cite{startOfXAI}. Since then, the number of publications in the XAI domain has witnessed exponential growth, demonstrating the increasing attention and significance given to this area of research. For instance, a bibliometric analysis conducted by Park and Lehman revealed a staggering 350\% rise in the number of XAI-related papers published between 2017 and 2020 \cite{analysisOfXAI}.

The significant growth of XAI leads to a notable diversity of approaches and techniques. This diversity is a result of researchers' varying perspectives and goals in their pursuit of improving the interpretability of AI systems. Various taxonomies have been suggested in the research literature to categorize different explainability methods \cite{taxonomy, medicalXAI}. However, it is important to note that these classification techniques are not fixed or absolute. They can differ significantly based on the specific characteristics of the methods, and methods may fall into multiple classes that overlap or do not overlap. In this context, different types of taxonomies and classification approaches will be briefly discussed, while a more comprehensive analysis of these taxonomies can be found in the referenced source \cite{taxonomy}.
\begin{figure}
\centering
\includegraphics[width=13cm]{images/xai_methods/taxonomy-of-XAI-methods.png}
\caption{Taxonomy of XAI methods \cite{taxonomy}}
\end{figure}

One notable variation is the distinction between interpretable-by-design and post-hoc XAI methods. Interpretable-by-design XAI methods involve incorporating interpretability during the model's development phase. These methods focus on designing inherently interpretable models, such as decision trees or rule-based systems, which provide explicit rules for decision-making. By building models with transparency in mind from the outset, interpretable-by-design methods offer direct interpretability without the need for additional post-hoc explanations. On the other hand, post-hoc XAI methods are applied after the model has been trained and are commonly used with black-box or complex models, including deep neural networks. These methods seek to explain the model's predictions without modifying its internal workings. Post-hoc XAI methods provide an additional layer of interpretability by generating explanations that are independent of the underlying model's architecture or learning algorithm.

Post-hoc XAI methods can be further categorized based on their approaches. A model-specific method is tailored to a particular machine learning model or architecture, leveraging its internal structure and characteristic to generate explanations. These methods include techniques like Layer-wise Relevance Propagation (LRP) \cite{lrp}, which rely on understanding the inner workings of deep neural networks. On the other hand, model-agnostic approaches, such as Local Interpretable Model-Agnostic Explanations (LIME) \cite{lime} and Shapley Additive Explanations (SHAP) \cite{shap}, aim to provide explanations that are independent of the underlying model. These methods generate explanations by approximating the model's behavior through sampling or perturbing the input data.

Another aspect of variation in XAI methods is the granularity of explanations. Some techniques focus on generating global explanations, providing insights into the overall behavior of the model across the entire dataset or feature space. Integrated Gradients \cite{IG} is an example of techniques that offer global explanations by assessing the impact of different features on model predictions. In contrast, local explanation methods concentrate on explaining individual predictions or instances. These methods, such as LIME \cite{lime} and SHAP \cite{shap}, highlight the specific features that influenced a particular prediction, providing more fine-grained explanations.

XAI methods also vary in terms of the types of explanations they generate. Some methods utilize textual or rule-based explanations to provide human-readable justifications for model decisions. Rule extraction techniques, for instance, aim to extract understandable rules from complex models, enabling users to comprehend the decision-making process. Other methods employ visualizations, such as saliency maps, to visually highlight important regions or features in the input data that contribute to model predictions. In this work, we will focus on this difference between explanations via saliency maps. 

\section{Overview of Saliency-based XAI Methods}
\label{sec:saliencyMapOverview}
The concept of saliency maps \cite{saliencyMaps} is based on the idea that certain regions or pixels in an image or input data play a crucial role in the model's prediction. By assigning importance scores to each pixel, saliency maps highlight the areas that have the highest influence on the model's output. This section outlines various saliency map explanation methods/algorithms that will be employed in our thesis. The methods can be categorized into two groups, depending on the approach they utilize to calculate the contribution of each input pixel value: Gradient-based and Perturbation-based.
 
\input{2_background/gradient-based_methods}
\input{2_background/perturb-based_methods}