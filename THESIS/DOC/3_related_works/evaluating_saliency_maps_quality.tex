\section{Evaluating the Quality of Saliency Maps}
\label{sec:evaluatingQualitySaliencyMaps}
Despite limited research on the consistency of explanations produced by saliency methods, there exists a considerable body of literature evaluating the efficacy of saliency maps. Saliency maps, in general, are considered a valuable tool for facilitating comprehension of black box systems. Notably, Alqaraawi et al. \cite{evaluatingSaliencyMaps} conducted an investigation into the potential application of saliency methods for non-expert users and demonstrated that saliency maps generated by the LRP algorithm can assist users in identifying image features that the black box model is sensitive to. However, the utility of saliency maps is not significant, and they may overlook critical features such as contrast, luminance, and others. The authors recommend using complementary global explanation methods in tandem with saliency maps to gain a more complete understanding of the complex black box system.

In medical imaging, saliency-based XAI methods play a predominant role \cite{xaiInMedicalImaging} and are usually go-to techniques when explaining for clinicians \cite{xaiInMedicalSurvey}. Since its introduction in 2013 as a visualization method for explaining ConvNets architectures \cite{saliencyMaps}, the application of saliency maps to explain image-related tasks in the medical field has become increasingly popular. As mentioned in Section \ref{sec:objective}, several studies have been conducted to evaluate whether current image explanation algorithms meet the accuracy requirements in the medical field, such as \cite{assistGrading, trustworthiness, fulfillRequirements}. Almost all of these studies share the same perspective as Cynthia Rudin's research \cite{xaiInMedicalSurvey}, which suggest that explanations generated by models should not be used in important tasks. According to experiment results, especially in image-related tasks in the medical field, saliency maps often exhibit similarities across different classification categories or are generally uninformative for end users \cite{xaiNotProvided}.

Concerns about the performance, robustness and fidelity of saliency methods have also been questioned. Nishanth et al. \cite{trustworthiness} found that many gradient-based methods demonstrate poor trustworthiness when used for high-stake domain of medical imaging. Eitel et al. \cite{testingAttributionMethodsRobustness} showed that Gradient*Input, Guided Backpropagation, LRP and Occlusion vary in robustness and produce inconsistent explanations for classifying Alzheimer's disease when subjected to repeated black box retraining. Saliency methods are also found to be independent from the model, but are sensitive to the data \cite{sanityCheck, interpretationOfNeuralNetworksIsFragile, unreliabilityOfSaliencyMethods}.

In summary, although saliency maps are very popular with multiple different techniques proposed, being an influential tools in many domains, there are still a lot of problems remain to be tackled regarding the usability of saliency maps.