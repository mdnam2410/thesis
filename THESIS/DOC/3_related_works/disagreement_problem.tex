\section{The Disagreement Problem}
\label{sec:disagreementProblem}
The problem of disagreement in the field of explainable artificial intelligence is a relatively recent issue. Brughmans et al. \cite{disagreementCounterfactual} characterized it as the occurrence of ``conflicting or contradictory explanations'' generated by different interpretation methods when assessing a given AI model. Roy et al. \cite{whyDontXAITechniquesAgree} referred to the disagreement between explanation methods as ``different (and even contradicting) explanations for the same model decisions''. Nonetheless, the precise extent of disagreement between explanations and the specific aspects of disagreement remains to be conclusively defined.

The study conducted by Neely et al. \cite{proneToDisagreement} is among the earliest works that have raised concerns about the issue of explanation disagreement. The authors utilized rank correlation, specifically Kendall's $\tau$ coefficient, as a measure to assess the degree of disagreement among several explanation methods including LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and attention-based explanations. The findings of the experiments indicated that there were notably low correlations observed between the results generated by the different methods, signifying that there is a high likelihood of disagreement among them.

Krishna et al. \cite{krishna_disagreement_problem} went further by formularizing the concept of disagreement problem based on real-world scenarios. Their research involved surveying practitioners, including data scientists and machine learning engineers, to investigate their use of XAI methods in their daily work, their experiences with disagreements among different methods, and how they address such discrepancies. The findings revealed that practitioners often employ multiple explanation methods rather than relying on a single one to comprehend the functioning of black box models. Furthermore, the practitioners' consideration of feature importance values varied, with factors such as mismatches in top features, feature order, and the sign of feature importance (whether positive or negative) taking precedence over the actual values generated by XAI methods. This observation is understandable given that XAI methods produce values with diverse meanings and properties. Using the above observations, the authors developed several metrics that capture the degree of disagreement between different XAI methods, which have laid the groundwork for further research on the problem. Our work is also influenced by this body of research.

Brughmans et al.'s study \cite{disagreementCounterfactual} investigates the issue of disagreement among counterfactual explanation algorithms, which describe a set of feature changes that can affect the predicted class of a black box model. In other words, such explanations explore the question of ``what would the prediction be if a particular feature were to change in a certain way?''. This approach is different from saliency maps in which counterfactual explanations explain the prediction using \emph{changes} in input features, while saliency maps interpret by \emph{attributing} each pixel a feature importance value. The research confirmed the presence of disagreement among counterfactual explanation methods and highlighted a notable discovery that the degree of disagreement is predominantly contingent on the dataset and methods utilized, rather than the type of classifiers employed.

\subsection{How is Disagreement Measured?}
\label{subsec:howIsDisagreementMeasured}
Since there is a lack of common evaluation frameworks, the measurement of disagreement is yet to be concretely defined and generally depends on the nature of the problem and the explanation methods chosen. Currently, there are two approaches to measuring disagreement: using set-based metrics and using correlation.

The set-based metrics involve finding a set of feature attributions that share some common characteristics. The measure of agreement (or disagreement) then is based on the size of that set. Krishna et al. \cite{krishna_disagreement_problem} developed some set-based metrics including feature agreement (using the set of shared features within the top-$k$ feature of both explanations), signed agreement, rank agreement, and signed rank agreement (shared top-$k$ features with the same signs, ranks, and both signs and ranks, respectively). Brughmanns et al. \cite{disagreementCounterfactual} adapted the set-based approach for counterfactual explanations with metrics such as relative feature exclusion, relative feature span, L0 distances, and feature disagreement (an improvement from Krishna et al.'s feature agreement that incorporates direction of disagreement). We refer to the original paper of the authors for a detailed description of these metrics.

The correlation approach sees Neely et al. \cite{proneToDisagreement} utilize Kendall's $\tau$ coefficient and Krishna et al. use Spearman's $\rho$ to measure the correlation between the relative ranking of the features.

In general, although differ in motivation, all the metrics above generally do not stress the importance of the values of the attribution, but instead, focus on the signs and the relative ordering of the features.

\subsection{Attempts in Resolving Disagreement}
\label{subsec:attemptsInResolvingDisagreement}
The attempts to address these disagreements have received scant attention in the literature. In practice, when faced with such disagreements, real-world practitioners often resort to selecting the method with which they are most familiar \cite{krishna_disagreement_problem}. \cite{whyDontXAITechniquesAgree} seeks to mitigate disagreement by presenting the user only with the features on which LIME and SHAP agree while ignoring those on which they disagree. While this approach may create the appearance of consistency for the user, it does not address the underlying issue. If employed incorrectly, this approach may lead to even greater levels of misinterpretation. For instance, in cases where two approaches exhibit significant disagreement, the subset of features on which they both agree may be too small and insignificant to provide a meaningful explanation of the model's prediction.