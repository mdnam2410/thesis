\section{Thesis Content}
\label{sec:thesisContent}
This thesis is organized into 6 chapters as followed:
\begin{itemize}
    \item In chapter \ref{ch:introduction} we introduce the context for our thesis and the problem that we focus on. We present the need for XAI in making complex models more interpretable to users and introduce saliency maps as a candidate solution. We then introduce the disagreement problem in which XAI methods differ from each other in explaining a complex black box. Finally, we state the goal of our thesis in attempting to study the disagreement between saliency methods, which currently lacks extensive research.
    \item In chapter \ref{ch:background} we provide an overview of XAI and some saliency-based methods. First, we cover XAI's history, growth, and classification of approaches in XAI. Then, we introduce two types of saliency-based explanation methods: gradient-based and perturbation-based and provide an overview of their general ideas.
    \item In chapter \ref{ch:relatedWorks} we discuss in detail the disagreement problem in XAI and introduce prior works that highlighted this problem. We also present several works on assessing the quality of saliency maps in practical use, since there is currently no work that highlights the disagreement between saliency-based methods specifically.
    \item In chapter \ref{ch:proposedMethod} we present our approach to quantifying the disagreement between saliency-based methods. First, we describe the mathematical formulation for four metrics we used to measure disagreement, including feature agreement, sign agreement, rank correlation (inherited and adapted for saliency maps from \cite{krishna_disagreement_problem}), and structural similarity index metric. Then, we select several methods (both gradient-based and perturbation-based) for generating explanations and two CNNs as black boxes (InceptionV3 and ResNet). We also provide an overview of the architecture of these two black boxes.
    \item In chapter \ref{ch:experimentsAndResult} we provide a detailed description of our experiment in measuring disagreement, as well as a discussion about our findings. First, we describe the dataset, the environment configurations we used, the two CNNs that we used as black boxes for our experiment, and how we measured disagreement. Then, we analyze the results and conclude some findings, which include a confirmation of disagreements among the explanation methods, a confirmation of complex disagreement patterns when evaluated on different black boxes, and an insight that the levels of disagreement are dependent on the kind of black box employed.
    \item In chapter \ref{ch:conclusion} we conclude our thesis by providing a summary of our key findings, identifying some drawbacks in our work, and discussing future directions toward resolving the disagreement problem.
\end{itemize}