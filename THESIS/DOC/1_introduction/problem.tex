\section{Problems}
\label{sec:problem} 
There are numerous methods available for providing explanations for the outcome of a black box, but there is a challenge in assessing the quality of these explanations. Without a standard evaluation framework, there is a risk of using explanations that are baseless, ambiguous, or even contradictory. This is known as the disagreement problem and has recently been identified as a concern. Studies have investigated the severity of this problem for various types of explanation methods, but to our knowledge, no research has been done on saliency maps.

The goal of XAI is to help users understand how complex AI systems work, thereby reducing incomprehensibility and increasing trust and confidence in the deployment of these systems. The disagreement problem poses a challenge to achieving this goal. If explanation methods do not agree on why the black box behaves in a certain way or provide conflicting explanations on what the black box considers important to its decision, it can cause confusion for users. Therefore, we believe this problem deserves the attention of XAI researchers and should be thoroughly studied.

% However, XAI itself faces a challenge - the disagreement between explanation methods for the same black-box model.  While this issue is important, it has not been extensively studied in general, particularly in the medical field. The research conducted by Krishna et al. \cite{krishna_disagreement_problem} is one of the first efforts to address this disagreement problem, demonstrating the inconsistency between XAI models based on feature importance across different types of data such as tabular, text, and image data.
