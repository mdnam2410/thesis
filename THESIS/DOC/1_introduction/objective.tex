\section{Objectives}
\label{sec:objective}
%Để đảm bảo độ tin cậy và hiệu quả của XAI trong lĩnh vực y tế, việc giải quyết vấn đề này là vô cùng quan trọng. Do đó, chúng tôi thực hiện nghiên cứu này nhằm đánh giá mức độ bất đồng giữa các phương pháp giải thích bằng cách tiến hành các thử nghiệm với nhiều phương pháp XAI khác nhau để giải thích một mô hình hộp đen trên tập dữ liệu về lĩnh vực y học.
While the disagreement problem has been recognized for various explanation methods, saliency maps have not been extensively studied in this regard, as mentioned in \ref{sec:problem}. Saliency methods are widely used to explain convolutional neural network (CNN) black boxes. With numerous techniques available and no consensus evaluation framework to guide their development, it is highly likely that some level of disagreement exists among these methods. Therefore, the purpose of this thesis is to investigate the disagreement problem among saliency methods. By doing so, we aim to shed light on various aspects of this problem and contribute to addressing it in the future.

% if we do not understand the reasons behind and address this issue, the results of deep learning models and their corresponding explanations will not be trustworthy, thereby potentially influencing many deep learning research outcomes. To tackle this issue, our study aims to thoroughly assess the extent of disagreement among explanation methods.

% Due to the limitations of time, our work does not directly provide a solution to this problem. Instead, our primary objective is to uncover the underlying causes of disagreement observed in different XAI methods when utilizing the saliency maps approach \cite{saliencyMaps} - focus on visualizing which parts of the input space attract attention and influence the model's output.

% There have been several previous studies raising concerns about the trustworthiness of saliency maps in the medical field \cite{trustworthiness} or whether such methods truly enhance the performance in explaining clinical outcomes \cite{assistGrading}. Their findings, however, suggest otherwise. By conducting experiments with various XAI methods to explain a black-box model on medical datasets by comparing saliency maps they produced, we believe that our research findings can contribute to identifying potential solutions for this issue.