\chapter{Conclusion}
\label{ch:conclusion}
In this chapter we conclude our thesis by providing a summary of our key findings, identify some drawbacks in our work and discuss future directions towards resolving the disagreement problem.

\section{Results}
This thesis delves into the disagreement problem in XAI in general and specifically examines the disagreement between explanation methods that utilize saliency maps, an area that has not been previously researched. Our study involved measuring the level of disagreement between \numExperimentedMethods\ saliency explanation methods over two black boxes, using four different metrics. Our findings can be summarized as follows:
\begin{itemize}
    \item We confirmed that there is a significant amount of disagreement in explanations between many saliency maps.
    \item We indicate that the level of inconsistency varies based on the type of black box employed.
    \item We also revealed that when applied to saliency maps, the sign agreement metric is roughly half of the feature agreement score, which may render it unnecessary for measuring disagreement.
\end{itemize}


\section{Limitations and Future Works}
\label{sec:futureWorks}
The disagreement problem is a novel issue that has yet to receive comprehensive research attention, resulting in a lack of diverse viewpoints and materials on the subject. However, we firmly believe that this is not a trivial issue and deserves greater attention. If not addressed adequately, the validity and transparency of XAI approaches, which are the key desired characteristics of XAI, may be seriously questioned. Our thesis is an attempt to explore a new aspect of this problem, and there is significant scope for further improvement.

We acknowledge some limitations of our work and suggest potential directions for future research. Firstly, we only tested the disagreement problem on a single dataset with one classification task. We believe that further researches on a wider range of datasets and black boxes will reveal many useful insights. Secondly, as we noted, the existing metrics are insufficient to capture the full extent of disagreement, and new metrics can be developed to address this limitation. Thirdly, while the existence of disagreement has been acknowledged, a thorough and systematic investigation into the reasons for disagreement has yet to be conducted. Future work should focus on uncovering the underlying reasons for this disagreement. Finally, we urge the XAI community to attach greater importance to this problem, as the goal of XAI is to assist humans in understanding machine learning and deep learning algorithms and to make them more interpretable, rather than introducing further confusion by providing inconsistent or contradictory explanations.
