\chapter*{Abstract}
\label{abstract}
Deep learning models have become increasingly complex and accurate, making them essential in various real-world applications like healthcare and security. Understanding the inner workings of these complex ``black box'' models is crucial for their appropriate use, creation, and trust. Explainable Artificial Intelligence (XAI) aims to make these complex systems interpretable through different methods, including saliency maps for convolutional neural networks. However, there is a growing concern about the ``disagreement problem,'' where explanation methods produce inconsistent and sometimes contradictory explanations. While this problem has been acknowledged for various explanation methods, disagreements within saliency methods have not been thoroughly explored. This thesis addresses this gap by measuring the extent of disagreement among popular saliency techniques using established metrics. We evaluated the disagreement for \numExperimentedMethods\ different saliency methods over two black boxes and found significant disagreement among saliency map explanations. Moreover, we observed complex patterns of disagreement, indicating that the disagreement problem depends not only on the type of explanation method but also on the specific model being explained. We also identified limitations in current measurement metrics and encourage researchers to develop new, robust metrics that can effectively capture various aspects of explanation disagreement.