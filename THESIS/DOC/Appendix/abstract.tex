\chapter*{Abstract}
\label{abstract}
Deep learning models have become increasingly complex and accurate, making them essential in various real-world applications like healthcare and security. Understanding the inner workings of these complex ``black box'' models is crucial for their appropriate use, creation, and trust. Explainable Artificial Intelligence (XAI) aims to make these complex systems interpretable through different methods, including saliency maps for convolutional neural networks. However, there is a growing concern about the ``disagreement problem'', where explanation methods produce inconsistent and sometimes contradictory explanations. While this problem has been acknowledged for various explanation methods, disagreements within saliency methods have not been thoroughly explored. This thesis addresses this gap by measuring the extent of disagreement among popular saliency techniques using established metrics. We evaluated the disagreement for \numExperimentedMethods\ different saliency methods over two black boxes and found significant disagreement among saliency map explanations. We also observed complex patterns of disagreement among the experimented methods, and finally we indicated that the level of disagreement depends on the kind of black box being explained.